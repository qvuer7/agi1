{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb7a4d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.agi.config import OPENROUTER_API_KEY, OPENROUTER_BASE_URL, OR_MODEL, DEFAULT_MAX_PAGES_FETCHED\n",
    "from src.agi.clients.openrouter_client import OpenRouterClient\n",
    "from src.agi.clients.brave_client import BraveClient\n",
    "from src.agi.clients.fetch_client import FetchClient\n",
    "from src.agi.cache.cache import Cache\n",
    "from src.agi.extract.html_extract import extract_text\n",
    "from src.agi.clients.browser_client import PlaywrightFetchClient\n",
    "\n",
    "# Final output construction\n",
    "import json\n",
    "import re\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "from bs4 import BeautifulSoup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "159f76f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenRouterClient()\n",
    "brave_client = BraveClient()\n",
    "fetch_client = FetchClient()\n",
    "seed = PlaywrightFetchClient(headless=False, user_data_dir=\".pw_sova\")\n",
    "cache = Cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a1e3cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "TOOLS = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"search_web\",\n",
    "            \"description\": \"Search the web for information. Use this first to find relevant URLs. For finding products on a specific site, use 'site:domain.com keywords' format (e.g., 'site:rozetka.ua iPhone 15').\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"query\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"Search query string. Use 'site:domain.com keywords' to search within a specific website.\",\n",
    "                    },\n",
    "                    \"count\": {\n",
    "                        \"type\": \"integer\",\n",
    "                        \"description\": \"Number of results to return (default: 5, max: 10)\",\n",
    "                        \"default\": 5,\n",
    "                        \"minimum\": 1,\n",
    "                        \"maximum\": 10,\n",
    "                    },\n",
    "                },\n",
    "                \"required\": [\"query\"],\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"fetch_url\",\n",
    "            \"description\": \"Fetch and extract text content from a URL. Use this after search_web to get page content. Use this to verify product pages.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"url\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"URL to fetch\",\n",
    "                    },\n",
    "                },\n",
    "                \"required\": [\"url\"],\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "408b0f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoupled tool execution functions\n",
    "\n",
    "def execute_search_web(query: str, count: int = 5):\n",
    "    \"\"\"Execute search_web tool.\"\"\"\n",
    "    # Check cache\n",
    "    cached = cache.get_search(query)\n",
    "    if cached:\n",
    "        results = cached\n",
    "    else:\n",
    "        try:\n",
    "            results = brave_client.search(query, count=count)\n",
    "            cache.set_search(query, results)\n",
    "        except Exception as e:\n",
    "            print(f\"Search failed: {e}\")\n",
    "            return {\n",
    "                \"content\": f\"Search failed: {e}\",\n",
    "                \"success\": False,\n",
    "            }\n",
    "    \n",
    "    if not results:\n",
    "        return {\n",
    "            \"content\": \"Search returned no results. Try a different query.\",\n",
    "            \"success\": False,\n",
    "        }\n",
    "    \n",
    "    # Format results\n",
    "    formatted = \"\\n\".join([\n",
    "        f\"- {r['title']}\\n  URL: {r['url']}\\n  {r['snippet']}\"\n",
    "        for r in results\n",
    "    ])\n",
    "    return {\n",
    "        \"content\": f\"Found {len(results)} search results:\\n\\n{formatted}\",\n",
    "        \"success\": True,\n",
    "    }\n",
    "\n",
    "\n",
    "def execute_fetch_url(url: str, fetched_urls: set, max_pages_fetched: int):\n",
    "    \"\"\"Execute fetch_url tool - basic HTTP fetch with status check.\"\"\"\n",
    "    if not url:\n",
    "        return {\"content\": \"No URL provided\", \"success\": False}\n",
    "    \n",
    "    if len(fetched_urls) >= max_pages_fetched:\n",
    "        return {\n",
    "            \"content\": f\"Maximum pages fetched ({max_pages_fetched}). Cannot fetch more.\",\n",
    "            \"success\": False,\n",
    "        }\n",
    "    \n",
    "    # Check cache\n",
    "    cached = cache.get_fetch(url)\n",
    "    if cached:\n",
    "        fetch_result = cached\n",
    "    else:\n",
    "        fetch_result = fetch_client.fetch(url)\n",
    "        if fetch_result.get(\"html\"):\n",
    "            # Extract text\n",
    "            text = extract_text(fetch_result[\"html\"])\n",
    "            fetch_result[\"text\"] = text\n",
    "            cache.set_fetch(url, fetch_result)\n",
    "    \n",
    "    status = fetch_result.get(\"status\", 0)\n",
    "    final_url = fetch_result.get(\"final_url\", url)\n",
    "    \n",
    "    # Basic status check - only fail on 404 or other errors\n",
    "    if status == 404:\n",
    "        return {\n",
    "            \"content\": f\"Fetched {url} returned 404 (not found).\",\n",
    "            \"success\": False,\n",
    "            \"url\": final_url,\n",
    "            \"status\": status,\n",
    "        }\n",
    "    \n",
    "    if status != 200:\n",
    "        error_msg = fetch_result.get(\"error\", \"Unknown error\")\n",
    "        return {\n",
    "            \"content\": f\"Fetched {url} returned status {status} (error: {error_msg}).\",\n",
    "            \"success\": False,\n",
    "            \"url\": final_url,\n",
    "            \"status\": status,\n",
    "            \"error\": error_msg,\n",
    "        }\n",
    "    \n",
    "    # Success - page fetched\n",
    "    fetched_urls.add(final_url)\n",
    "    text = fetch_result.get(\"text\", extract_text(fetch_result.get(\"html\", \"\")))\n",
    "    title = fetch_result.get(\"title\", \"\")\n",
    "    extracted_links = fetch_result.get(\"extracted_links\", [])\n",
    "    canonical_url = fetch_result.get(\"canonical_url\")\n",
    "    \n",
    "    # Format content\n",
    "    content = f\"Fetched {url}:\\n\\n{text}\"\n",
    "    if extracted_links:\n",
    "        content += f\"\\n\\nFound {len(extracted_links)} links on this page:\\n\"\n",
    "        for i, link in enumerate(extracted_links[:10], 1):  # Show first 10\n",
    "            content += f\"{i}. {link}\\n\"\n",
    "        if len(extracted_links) > 10:\n",
    "            content += f\"... and {len(extracted_links) - 10} more\\n\"\n",
    "    \n",
    "    return {\n",
    "        \"content\": content,\n",
    "        \"success\": True,\n",
    "        \"url\": final_url,\n",
    "        \"final_url\": final_url,\n",
    "        \"canonical_url\": canonical_url,\n",
    "        \"title\": title,\n",
    "        \"extracted_links\": extracted_links,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e4fd23",
   "metadata": {},
   "source": [
    "### Phase 1 product research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68fb3bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://sovajewels.com/catalog/koltsa/koltso-iz-belogo-zolota-i-keramiki-smart-beautiful-artikul-110474820202.html'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43ef8ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# impossible to use in jupyter dut to async api of playwright\n",
    "# res = seed.fetch(url)\n",
    "\n",
    "# loading rendered .html file:\n",
    "with open('debug_sova_seed.html', 'r', encoding='utf-8') as f:\n",
    "    data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f4cb295",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def strip_html_basic(html: str) -> str:\n",
    "    \"\"\"\n",
    "    Minimal HTML cleaner:\n",
    "    - removes script/style/noscript/svg/iframe/canvas\n",
    "    - removes comments\n",
    "    - returns cleaned HTML string\n",
    "    \"\"\"\n",
    "    if not html:\n",
    "        return \"\"\n",
    "\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "    # remove heavy / irrelevant tags\n",
    "    for tag in soup([\"script\", \"style\", \"noscript\", \"svg\", \"iframe\", \"canvas\"]):\n",
    "        tag.decompose()\n",
    "\n",
    "    # remove HTML comments\n",
    "    for c in soup.find_all(string=lambda s: isinstance(s, type(soup.comment))):\n",
    "        c.extract()\n",
    "\n",
    "    return str(soup)\n",
    "\n",
    "\n",
    "def html_to_text_basic(html: str, max_chars: int = 12000) -> str:\n",
    "    \"\"\"\n",
    "    Minimal text extraction:\n",
    "    - strips junk tags\n",
    "    - uses get_text() to flatten\n",
    "    - trims length\n",
    "    \"\"\"\n",
    "    cleaned = strip_html_basic(html)\n",
    "    if not cleaned:\n",
    "        return \"\"\n",
    "\n",
    "    soup = BeautifulSoup(cleaned, \"lxml\")\n",
    "    text = soup.get_text(separator=\"\\n\", strip=True)\n",
    "\n",
    "    # collapse excessive newlines\n",
    "    lines = [line.strip() for line in text.splitlines() if line.strip()]\n",
    "    text = \"\\n\".join(lines)\n",
    "\n",
    "    return text[:max_chars]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97096763",
   "metadata": {},
   "outputs": [],
   "source": [
    "databasic = strip_html_basic(data)\n",
    "textbasic = html_to_text_basic(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt = textbasic\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": (\n",
    "            \"You are a product information extractor.\\n\"\n",
    "            \"You receive plain text extracted from an HTML product page.\\n\"\n",
    "            \"Your task is to identify ONE main product and extract its key information.\\n\\n\"\n",
    "\n",
    "            \"OUTPUT RULES:\\n\"\n",
    "            \"- Return ONLY valid JSON.\\n\"\n",
    "            \"- Do NOT add explanations.\\n\"\n",
    "            \"- Do NOT add markdown.\\n\"\n",
    "            \"- Do NOT invent information.\\n\"\n",
    "            \"- If some field is not present, return an empty string or null.\\n\\n\"\n",
    "\n",
    "            \"REQUIRED JSON FORMAT:\\n\"\n",
    "            \"{\\n\"\n",
    "            '  \\\"title\\\": \\\"\\\",\\n'\n",
    "            '  \\\"price\\\": null,\\n'\n",
    "            '  \\\"currency\\\": \\\"\\\",\\n'\n",
    "            '  \\\"characteristics\\\": [\\n'\n",
    "            '    \\\"\\\",\\n'\n",
    "            '    \\\"\\\",\\n'\n",
    "            '    \\\"\\\"\\n'\n",
    "            \"  ]\\n\"\n",
    "            \"}\\n\\n\"\n",
    "\n",
    "            \"FIELD DEFINITIONS:\\n\"\n",
    "            \"- title: main product name as shown in the text\\n\"\n",
    "            \"- price: numeric value of the main product price (no currency symbols)\\n\"\n",
    "            \"- currency: currency code or symbol (e.g. UAH, USD, EUR, ₴, $, €)\\n\"\n",
    "            \"- characteristics: 3 to 6 most important technical or descriptive attributes \"\n",
    "            \"(material, size, model, color, features, compatibility, etc.)\\n\"\n",
    "        ),\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": user_prompt},\n",
    "]\n",
    "\n",
    "out = client.chat(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b196d969",
   "metadata": {},
   "source": [
    "### Phase 2 product research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c4ba869",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, re\n",
    "\n",
    "def parse_llm_json(content: str) -> dict:\n",
    "    m = re.search(r\"```json\\s*({.*?})\\s*```\", content, re.DOTALL | re.IGNORECASE)\n",
    "    if m:\n",
    "        return json.loads(m.group(1))\n",
    "    # fallback: try parse whole content\n",
    "    return json.loads(content)\n",
    "\n",
    "ref_json = parse_llm_json(out[\"content\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e27f01bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_domain = \"zolotiyvik.ua\"\n",
    "user_prompt = json.dumps({\n",
    "    \"target_domain\": target_domain,\n",
    "    \"reference\": ref_json,   # <-- your phase1 JSON dict\n",
    "}, ensure_ascii=False)\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": (\n",
    "            \"You are a product search agent.\\n\"\n",
    "            \"Input: JSON with (target_domain, reference).\\n\"\n",
    "            \"Tools: search_web (Brave).\\n\\n\"\n",
    "\n",
    "            \"TASK:\\n\"\n",
    "            \"Using ONLY search_web, find candidate URLs on target_domain that are likely to contain products similar to the reference.\\n\"\n",
    "            \"Your output for now is discovery only (no page fetching).\\n\\n\"\n",
    "\n",
    "            \"RULES:\\n\"\n",
    "            \"- Use ONLY URLs returned by search_web.\\n\"\n",
    "            \"- Always restrict queries to target_domain using: site:TARGET_DOMAIN\\n\"\n",
    "            \"- Do NOT assume any domain-specific vocabulary.\\n\"\n",
    "            \"- Generate queries using the reference fields:\\n\"\n",
    "            \"  - title\\n\"\n",
    "            \"  - characteristics (as free-text tokens)\\n\"\n",
    "            \"  - price + currency (use as optional constraints)\\n\"\n",
    "            \"- Prefer broader queries first, then narrower.\\n\"\n",
    "            \"- Run 6 to 10 search_web calls maximum.\\n\"\n",
    "            \"- Collect and deduplicate URLs.\\n\"\n",
    "            \"- Guess url_type from snippet+path only: one of [\\\"listing\\\", \\\"product\\\", \\\"other\\\", \\\"unknown\\\"].\\n\\n\"\n",
    "\n",
    "            \"QUERY STRATEGY:\\n\"\n",
    "            \"1) Start broad: site:TARGET_DOMAIN + short title keywords (2-4 words)\\n\"\n",
    "            \"2) Add 1-2 strongest characteristics (keep them as-is; do not normalize)\\n\"\n",
    "            \"3) If results are too broad, add price as a hint (optional)\\n\"\n",
    "            \"4) Try language variants only by removing/adding punctuation and using substrings, not translations.\\n\\n\"\n",
    "\n",
    "            \"OUTPUT (RETURN ONLY VALID JSON):\\n\"\n",
    "            \"{\\n\"\n",
    "            \"  \\\"target_domain\\\": \\\"\\\",\\n\"\n",
    "            \"  \\\"queries_used\\\": [\\\"...\\\"],\\n\"\n",
    "            \"  \\\"candidates\\\": [\\n\"\n",
    "            \"    {\\n\"\n",
    "            \"      \\\"url\\\": \\\"\\\",\\n\"\n",
    "            \"      \\\"title\\\": \\\"\\\",\\n\"\n",
    "            \"      \\\"snippet\\\": \\\"\\\",\\n\"\n",
    "            \"      \\\"url_type_guess\\\": \\\"listing|product|other|unknown\\\",\\n\"\n",
    "            \"      \\\"matched_reference_bits\\\": [\\\"...\\\"],\\n\"\n",
    "            \"      \\\"evidence_query\\\": \\\"...\\\"\\n\"\n",
    "            \"    }\\n\"\n",
    "            \"  ]\\n\"\n",
    "            \"}\\n\"\n",
    "        ),\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": user_prompt},\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "16614976",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = client.chat(messages, tools = TOOLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dcd4fd69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'role': 'assistant',\n",
       " 'content': '',\n",
       " 'refusal': None,\n",
       " 'reasoning': 'The user wants me to search for products similar to a reference on the domain zolotiyvik.ua. The reference is a ring made of white gold and ceramic called \"Smart & Beautiful\" with price 6664 UAH.\\n\\nLet me analyze the reference:\\n- Title: Кольцо из белого золота и керамики Smart & Beautiful\\n- Price: 6664 UAH\\n- Characteristics:\\n  - Металл: Золото 585 пробы\\n  - Цвет металла: Белый\\n  - Материал: Керамика\\n  - Средний вес: 2.2107 г\\n  - Артикул: 110474820201\\n\\nI need to generate 6-10 search queries using site:zolotiyvik.ua with different combinations of keywords:\\n\\n1. Broad: site:zolotiyvik.ua кольцо золото керамика\\n2. With white gold: site:zolotiyvik.ua белое золото керамика кольцо\\n3. With the brand/name: site:zolotiyvik.ua Smart Beautiful кольцо\\n4. With article number: site:zolotiyvik.ua 110474820201\\n5. White gold ring ceramic: site:zolotiyvik.ua кольцо белое золото керамика 585\\n6. With price hint: site:zolotiyvik.ua кольцо керамика 6664\\n\\nLet me start with these queries.',\n",
       " 'tool_calls': [{'type': 'function',\n",
       "   'index': 0,\n",
       "   'id': 'search_web:0',\n",
       "   'function': {'name': 'search_web',\n",
       "    'arguments': '{\"query\": \"site:zolotiyvik.ua кольцо золото керамика\", \"count\": 10}'}},\n",
       "  {'type': 'function',\n",
       "   'index': 1,\n",
       "   'id': 'search_web:1',\n",
       "   'function': {'name': 'search_web',\n",
       "    'arguments': '{\"query\": \"site:zolotiyvik.ua белое золото керамика кольцо\", \"count\": 10}'}},\n",
       "  {'type': 'function',\n",
       "   'index': 2,\n",
       "   'id': 'search_web:2',\n",
       "   'function': {'name': 'search_web',\n",
       "    'arguments': '{\"query\": \"site:zolotiyvik.ua Smart Beautiful кольцо\", \"count\": 10}'}},\n",
       "  {'type': 'function',\n",
       "   'index': 3,\n",
       "   'id': 'search_web:3',\n",
       "   'function': {'name': 'search_web',\n",
       "    'arguments': '{\"query\": \"site:zolotiyvik.ua 110474820201\", \"count\": 10}'}},\n",
       "  {'type': 'function',\n",
       "   'index': 4,\n",
       "   'id': 'search_web:4',\n",
       "   'function': {'name': 'search_web',\n",
       "    'arguments': '{\"query\": \"site:zolotiyvik.ua кольцо золото 585 керамика\", \"count\": 10}'}},\n",
       "  {'type': 'function',\n",
       "   'index': 5,\n",
       "   'id': 'search_web:5',\n",
       "   'function': {'name': 'search_web',\n",
       "    'arguments': '{\"query\": \"site:zolotiyvik.ua кольцо керамика 6664\", \"count\": 10}'}}],\n",
       " 'reasoning_details': [{'format': 'unknown',\n",
       "   'index': 0,\n",
       "   'type': 'reasoning.text',\n",
       "   'text': 'The user wants me to search for products similar to a reference on the domain zolotiyvik.ua. The reference is a ring made of white gold and ceramic called \"Smart & Beautiful\" with price 6664 UAH.\\n\\nLet me analyze the reference:\\n- Title: Кольцо из белого золота и керамики Smart & Beautiful\\n- Price: 6664 UAH\\n- Characteristics:\\n  - Металл: Золото 585 пробы\\n  - Цвет металла: Белый\\n  - Материал: Керамика\\n  - Средний вес: 2.2107 г\\n  - Артикул: 110474820201\\n\\nI need to generate 6-10 search queries using site:zolotiyvik.ua with different combinations of keywords:\\n\\n1. Broad: site:zolotiyvik.ua кольцо золото керамика\\n2. With white gold: site:zolotiyvik.ua белое золото керамика кольцо\\n3. With the brand/name: site:zolotiyvik.ua Smart Beautiful кольцо\\n4. With article number: site:zolotiyvik.ua 110474820201\\n5. White gold ring ceramic: site:zolotiyvik.ua кольцо белое золото керамика 585\\n6. With price hint: site:zolotiyvik.ua кольцо керамика 6664\\n\\nLet me start with these queries.'}]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbaed7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_calls = out.get(\"tool_calls\", [])\n",
    "tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8469553",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_messages = []\n",
    "url_provenance = {}\n",
    "sources = []\n",
    "rejected_urls = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ed4d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tool_calls = out.get(\"tool_calls\", [])\n",
    "for tool_call in tool_calls:\n",
    "    function_name = tool_call.get(\"function\", {}).get(\"name\", \"\")\n",
    "    function_args_str = tool_call.get(\"function\", {}).get(\"arguments\", \"{}\")\n",
    "\n",
    "    try:\n",
    "        import json\n",
    "        function_args = json.loads(function_args_str)\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Invalid tool call arguments: {function_args_str}\")\n",
    "        continue\n",
    "\n",
    "    print(f\"Tool call: {function_name}({function_args})\")\n",
    "\n",
    "    # Execute tool using decoupled functions\n",
    "    if function_name == \"search_web\":\n",
    "        query = function_args.get(\"query\", \"\")\n",
    "        count = function_args.get(\"count\", 5)\n",
    "        tool_result = execute_search_web(query, count)\n",
    " \n",
    "    new_messages.append({\n",
    "        \"role\": \"tool\",\n",
    "        \"tool_call_id\": tool_call.get(\"id\"),\n",
    "        \"content\": tool_result[\"content\"],\n",
    "    })\n",
    "\n",
    "    # Track sources and rejected URLs\n",
    "    if \"url\" in tool_result:\n",
    "        url = tool_result.get(\"final_url\") or tool_result.get(\"url\")\n",
    "        \n",
    "        if tool_result.get(\"success\"):\n",
    "            # Successful fetch\n",
    "            title = tool_result.get(\"title\", url)\n",
    "            \n",
    "            # Update provenance (simplified)\n",
    "            if url not in url_provenance:\n",
    "                url_provenance[url] = {\n",
    "                    \"url\": url,\n",
    "                    \"title\": title,\n",
    "                    \"final_url\": tool_result.get(\"final_url\", url),\n",
    "                    \"canonical_url\": tool_result.get(\"canonical_url\"),\n",
    "                }\n",
    "            \n",
    "            # Add to sources if not already present\n",
    "            if not any(s.get(\"url\") == url for s in sources):\n",
    "                sources.append({\n",
    "                    \"url\": url,\n",
    "                    \"title\": title,\n",
    "                })\n",
    "        else:\n",
    "            # Failed fetch - track as rejected\n",
    "            reason = tool_result.get(\"content\", \"Unknown error\")\n",
    "            status = tool_result.get(\"status\", \"unknown\")\n",
    "            rejected_urls.append({\n",
    "                \"url\": url,\n",
    "                \"reason\": f\"Status {status}: {reason}\",\n",
    "            })\n",
    "\n",
    "    # Track debug info\n",
    "    debug_traces.append({\n",
    "        \"step\": step + 1,\n",
    "        \"tool\": function_name,\n",
    "        \"args\": function_args,\n",
    "        \"result_length\": len(tool_result.get(\"content\", \"\")),\n",
    "        \"success\": tool_result.get(\"success\", False),\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62eb2c64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a58c47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62eaac6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "34186a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "TOOLS = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"search_web\",\n",
    "            \"description\": \"Search the web for information. Use this first to find relevant URLs. For finding products on a specific site, use 'site:domain.com keywords' format (e.g., 'site:rozetka.ua iPhone 15').\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"query\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"Search query string. Use 'site:domain.com keywords' to search within a specific website.\",\n",
    "                    },\n",
    "                    \"count\": {\n",
    "                        \"type\": \"integer\",\n",
    "                        \"description\": \"Number of results to return (default: 5, max: 10)\",\n",
    "                        \"default\": 5,\n",
    "                        \"minimum\": 1,\n",
    "                        \"maximum\": 10,\n",
    "                    },\n",
    "                },\n",
    "                \"required\": [\"query\"],\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"fetch_url\",\n",
    "            \"description\": \"Fetch and extract text content from a URL. Use this after search_web to get page content. Use this to verify product pages.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"url\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"URL to fetch\",\n",
    "                    },\n",
    "                },\n",
    "                \"required\": [\"url\"],\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1f7a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt = \"найди 5 максимально похожих товаров на этот: https://sovajewels.com/ua/p/koltso-iz-belogo-zolota-i-keramiki-smart-beautiful-artikul-110474820202/ только на сайте этой кмпании: https://zolotiyvik.ua/ua/\"\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": (\n",
    "            \"You are a product research assistant.\\n\"\n",
    "            \"Tools: fetch_url (HTTP GET), search_web (Brave search).\\n\\n\"\n",
    "\n",
    "            \"PHASE 1 ONLY: Reference product -> MAIN DEFINITIONS\\n\"\n",
    "            \"Goal: Extract MAIN DEFINITIONS (MIN 2, MAX 10) that best define the product for later matching.\\n\\n\"\n",
    "\n",
    "            \"Algorithm:\\n\"\n",
    "            \"1) From the user message, extract reference_url.\\n\"\n",
    "            \"2) Try fetch_url(reference_url).\\n\"\n",
    "            \"3) If fetch_url fails (success=false OR status!=200), fallback to search_web:\\n\"\n",
    "            \"   - run search_web using reference_url as query\\n\"\n",
    "            \"   - also run search_web using tokens from the URL slug/title (if obvious)\\n\"\n",
    "            \"   - from search results, pick up to 2 URLs that likely describe the same product and try fetch_url on them\\n\"\n",
    "            \"4) Build product definitions from the best available source(s) in this priority order:\\n\"\n",
    "            \"   - successful fetch_url text\\n\"\n",
    "            \"   - search_web snippets\\n\"\n",
    "            \"5) MAIN DEFINITIONS must be specific/actionable attributes (avoid vague terms).\\n\"\n",
    "            \"   Examples: color, size/dimensions, capacity,price_range, material, key feature, compatibility, form-factor.\\n\"\n",
    "            \"6) If something is inferred (not explicitly stated), mark it as uncertain.\\n\\n\"\n",
    "\n",
    "            \"GLOBAL RULES:\\n\"\n",
    "            \"- Do NOT invent URLs.\\n\"\n",
    "            \"- Only use URLs from the user input or returned by search_web.\\n\"\n",
    "            \"- Always include evidence_urls (exact URLs used).\\n\\n\"\n",
    "\n",
    "            \"FINAL OUTPUT: Return ONLY valid JSON (no markdown, no extra text).\\n\"\n",
    "            \"{\\n\"\n",
    "            '  \"phase\": \"phase_1_reference_definitions\",\\n'\n",
    "            '  \"reference_url\": \"\",\\n'\n",
    "            '  \"fetch_attempts\": [\\n'\n",
    "            '    {\"url\": \"\", \"success\": false, \"status\": null}\\n'\n",
    "            \"  ],\\n\"\n",
    "            '  \"source\": \"fetch_url|search_web|mixed\",\\n'\n",
    "            '  \"product\": {\\n'\n",
    "            '    \"title\": \"\",\\n'\n",
    "            '    \"product_type\": \"\",\\n'\n",
    "            '    \"brand\": \"\",\\n'\n",
    "            '    \"model\": \"\",\\n'\n",
    "            '    \"main_definitions\": [\\n'\n",
    "            '      {\"name\": \"\", \"value\": \"\", \"source\": \"fetch|search\", \"uncertain\": false}\\n'\n",
    "            \"    ],\\n\"\n",
    "            '    \"keywords\": []\\n'\n",
    "            \"  },\\n\"\n",
    "            '  \"evidence_urls\": []\\n'\n",
    "            \"}\\n\"\n",
    "        ),\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": user_prompt},\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ee84f6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoupled tool execution functions\n",
    "\n",
    "def execute_search_web(query: str, count: int = 5):\n",
    "    \"\"\"Execute search_web tool.\"\"\"\n",
    "    # Check cache\n",
    "    cached = cache.get_search(query)\n",
    "    if cached:\n",
    "        results = cached\n",
    "    else:\n",
    "        try:\n",
    "            results = brave_client.search(query, count=count)\n",
    "            cache.set_search(query, results)\n",
    "        except Exception as e:\n",
    "            print(f\"Search failed: {e}\")\n",
    "            return {\n",
    "                \"content\": f\"Search failed: {e}\",\n",
    "                \"success\": False,\n",
    "            }\n",
    "    \n",
    "    if not results:\n",
    "        return {\n",
    "            \"content\": \"Search returned no results. Try a different query.\",\n",
    "            \"success\": False,\n",
    "        }\n",
    "    \n",
    "    # Format results\n",
    "    formatted = \"\\n\".join([\n",
    "        f\"- {r['title']}\\n  URL: {r['url']}\\n  {r['snippet']}\"\n",
    "        for r in results\n",
    "    ])\n",
    "    return {\n",
    "        \"content\": f\"Found {len(results)} search results:\\n\\n{formatted}\",\n",
    "        \"success\": True,\n",
    "    }\n",
    "\n",
    "\n",
    "def execute_fetch_url(url: str, fetched_urls: set, max_pages_fetched: int):\n",
    "    \"\"\"Execute fetch_url tool - basic HTTP fetch with status check.\"\"\"\n",
    "    if not url:\n",
    "        return {\"content\": \"No URL provided\", \"success\": False}\n",
    "    \n",
    "    if len(fetched_urls) >= max_pages_fetched:\n",
    "        return {\n",
    "            \"content\": f\"Maximum pages fetched ({max_pages_fetched}). Cannot fetch more.\",\n",
    "            \"success\": False,\n",
    "        }\n",
    "    \n",
    "    # Check cache\n",
    "    cached = cache.get_fetch(url)\n",
    "    if cached:\n",
    "        fetch_result = cached\n",
    "    else:\n",
    "        fetch_result = fetch_client.fetch(url)\n",
    "        if fetch_result.get(\"html\"):\n",
    "            # Extract text\n",
    "            text = extract_text(fetch_result[\"html\"])\n",
    "            fetch_result[\"text\"] = text\n",
    "            cache.set_fetch(url, fetch_result)\n",
    "    \n",
    "    status = fetch_result.get(\"status\", 0)\n",
    "    final_url = fetch_result.get(\"final_url\", url)\n",
    "    \n",
    "    # Basic status check - only fail on 404 or other errors\n",
    "    if status == 404:\n",
    "        return {\n",
    "            \"content\": f\"Fetched {url} returned 404 (not found).\",\n",
    "            \"success\": False,\n",
    "            \"url\": final_url,\n",
    "            \"status\": status,\n",
    "        }\n",
    "    \n",
    "    if status != 200:\n",
    "        error_msg = fetch_result.get(\"error\", \"Unknown error\")\n",
    "        return {\n",
    "            \"content\": f\"Fetched {url} returned status {status} (error: {error_msg}).\",\n",
    "            \"success\": False,\n",
    "            \"url\": final_url,\n",
    "            \"status\": status,\n",
    "            \"error\": error_msg,\n",
    "        }\n",
    "    \n",
    "    # Success - page fetched\n",
    "    fetched_urls.add(final_url)\n",
    "    text = fetch_result.get(\"text\", extract_text(fetch_result.get(\"html\", \"\")))\n",
    "    title = fetch_result.get(\"title\", \"\")\n",
    "    extracted_links = fetch_result.get(\"extracted_links\", [])\n",
    "    canonical_url = fetch_result.get(\"canonical_url\")\n",
    "    \n",
    "    # Format content\n",
    "    content = f\"Fetched {url}:\\n\\n{text}\"\n",
    "    if extracted_links:\n",
    "        content += f\"\\n\\nFound {len(extracted_links)} links on this page:\\n\"\n",
    "        for i, link in enumerate(extracted_links[:10], 1):  # Show first 10\n",
    "            content += f\"{i}. {link}\\n\"\n",
    "        if len(extracted_links) > 10:\n",
    "            content += f\"... and {len(extracted_links) - 10} more\\n\"\n",
    "    \n",
    "    return {\n",
    "        \"content\": content,\n",
    "        \"success\": True,\n",
    "        \"url\": final_url,\n",
    "        \"final_url\": final_url,\n",
    "        \"canonical_url\": canonical_url,\n",
    "        \"title\": title,\n",
    "        \"extracted_links\": extracted_links,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e913f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "from typing import Any, Dict, List, Optional\n",
    "def extract_json_from_text(text: str) -> str:\n",
    "    \"\"\"Extract a JSON object from model output (handles markdown code blocks).\"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "\n",
    "    # Prefer fenced JSON blocks\n",
    "    m = re.search(r\"```(?:json)?\\s*({[\\s\\S]*?})\\s*```\", text, re.IGNORECASE)\n",
    "    if m:\n",
    "        return m.group(1).strip()\n",
    "\n",
    "    # Otherwise try to capture the first top-level JSON object.\n",
    "    # This is safer than greedy {.*} when the output contains braces in text.\n",
    "    start = text.find(\"{\")\n",
    "    if start == -1:\n",
    "        return text.strip()\n",
    "\n",
    "    # Scan to find matching closing brace for the first object\n",
    "    depth = 0\n",
    "    in_str = False\n",
    "    esc = False\n",
    "    for i in range(start, len(text)):\n",
    "        ch = text[i]\n",
    "        if in_str:\n",
    "            if esc:\n",
    "                esc = False\n",
    "            elif ch == \"\\\\\":\n",
    "                esc = True\n",
    "            elif ch == '\"':\n",
    "                in_str = False\n",
    "        else:\n",
    "            if ch == '\"':\n",
    "                in_str = True\n",
    "            elif ch == \"{\":\n",
    "                depth += 1\n",
    "            elif ch == \"}\":\n",
    "                depth -= 1\n",
    "                if depth == 0:\n",
    "                    return text[start:i + 1].strip()\n",
    "\n",
    "    # Fallback: return from first '{' to end\n",
    "    return text[start:].strip()\n",
    "\n",
    "\n",
    "def _extract_first_url(user_prompt: str) -> Optional[str]:\n",
    "    m = re.search(r\"https?://[^\\s]+\", user_prompt or \"\")\n",
    "    return m.group(0) if m else None\n",
    "\n",
    "\n",
    "def construct_final_output(\n",
    "    answer: str,\n",
    "    user_prompt: str,\n",
    "    sources: List[Dict[str, Any]],\n",
    "    rejected_urls: List[Dict[str, Any]],\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"Construct Phase-1 final output JSON: phase_1_reference_definitions.\"\"\"\n",
    "    parsed: Optional[Dict[str, Any]] = None\n",
    "\n",
    "    # Parse model output as JSON\n",
    "    try:\n",
    "        json_text = extract_json_from_text(answer)\n",
    "        parsed = json.loads(json_text) if json_text else None\n",
    "        if not isinstance(parsed, dict):\n",
    "            parsed = None\n",
    "    except Exception as e:\n",
    "        print(f\"Could not parse LLM output as JSON: {e}\")\n",
    "        parsed = None\n",
    "\n",
    "    # Extract reference_url from user prompt as a fallback\n",
    "    reference_url = _extract_first_url(user_prompt)\n",
    "\n",
    "    # Collect evidence urls from tool calls\n",
    "    tool_urls = [s.get(\"url\") for s in (sources or []) if s.get(\"url\")]\n",
    "    tool_urls = [u for u in tool_urls if isinstance(u, str) and u.strip()]\n",
    "\n",
    "    # Defaults for phase 1\n",
    "    default_output: Dict[str, Any] = {\n",
    "        \"phase\": \"phase_1_reference_definitions\",\n",
    "        \"reference_url\": reference_url or \"\",\n",
    "        \"fetch_attempts\": [],\n",
    "        \"source\": \"mixed\" if tool_urls else \"search_web\",\n",
    "        \"product\": {\n",
    "            \"title\": \"\",\n",
    "            \"product_type\": \"\",\n",
    "            \"brand\": \"\",\n",
    "            \"model\": \"\",\n",
    "            \"main_definitions\": [],\n",
    "            \"keywords\": [],\n",
    "        },\n",
    "        \"evidence_urls\": [],\n",
    "        # Optional passthrough for your pipeline (even if unused now)\n",
    "        \"rejected\": rejected_urls or [],\n",
    "    }\n",
    "\n",
    "    if not parsed:\n",
    "        # No valid model JSON -> return defaults + tool evidence\n",
    "        evidence = list(dict.fromkeys(default_output[\"evidence_urls\"] + tool_urls))\n",
    "        default_output[\"evidence_urls\"] = evidence\n",
    "        return default_output\n",
    "\n",
    "    # Start from defaults, then merge parsed fields cautiously\n",
    "    out = default_output\n",
    "\n",
    "    # Phase\n",
    "    if isinstance(parsed.get(\"phase\"), str) and parsed[\"phase\"].strip():\n",
    "        out[\"phase\"] = parsed[\"phase\"].strip()\n",
    "\n",
    "    # reference_url\n",
    "    if isinstance(parsed.get(\"reference_url\"), str) and parsed[\"reference_url\"].strip():\n",
    "        out[\"reference_url\"] = parsed[\"reference_url\"].strip()\n",
    "\n",
    "    # fetch_attempts\n",
    "    if isinstance(parsed.get(\"fetch_attempts\"), list):\n",
    "        out[\"fetch_attempts\"] = parsed[\"fetch_attempts\"]\n",
    "\n",
    "    # source\n",
    "    if isinstance(parsed.get(\"source\"), str) and parsed[\"source\"].strip():\n",
    "        out[\"source\"] = parsed[\"source\"].strip()\n",
    "\n",
    "    # product\n",
    "    if isinstance(parsed.get(\"product\"), dict):\n",
    "        p = parsed[\"product\"]\n",
    "        out_p = out[\"product\"]\n",
    "\n",
    "        for k in [\"title\", \"product_type\", \"brand\", \"model\"]:\n",
    "            if isinstance(p.get(k), str):\n",
    "                out_p[k] = p[k]\n",
    "\n",
    "        if isinstance(p.get(\"keywords\"), list):\n",
    "            out_p[\"keywords\"] = p[\"keywords\"]\n",
    "\n",
    "        if isinstance(p.get(\"main_definitions\"), list):\n",
    "            out_p[\"main_definitions\"] = p[\"main_definitions\"]\n",
    "\n",
    "        out[\"product\"] = out_p\n",
    "\n",
    "    # evidence_urls: merge parsed + tool urls (dedup, keep order)\n",
    "    parsed_evidence = parsed.get(\"evidence_urls\")\n",
    "    if not isinstance(parsed_evidence, list):\n",
    "        parsed_evidence = []\n",
    "    merged = list(dict.fromkeys([u for u in parsed_evidence if isinstance(u, str)] + tool_urls))\n",
    "    out[\"evidence_urls\"] = merged\n",
    "\n",
    "    # Keep rejected passthrough\n",
    "    out[\"rejected\"] = rejected_urls or parsed.get(\"rejected\", []) or []\n",
    "\n",
    "    # Ensure reference_url is present\n",
    "    if not out.get(\"reference_url\") and reference_url:\n",
    "        out[\"reference_url\"] = reference_url\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f47209f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_steps = 10\n",
    "fetched_urls = set()\n",
    "sources = []\n",
    "debug_traces = []\n",
    "url_provenance = {}\n",
    "rejected_urls = []  # Track URLs that failed (404, errors, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34968e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = \"\"  # Initialize answer variable\n",
    "\n",
    "for step in range(max_steps):\n",
    "    print(f\"Agent step {step + 1}/{max_steps}\")\n",
    "\n",
    "    # Call LLM\n",
    "    try:\n",
    "        assistant_message = client.chat(messages, tools=TOOLS)\n",
    "    except Exception as e:\n",
    "        print(f\"OpenRouter call failed: {e}\")\n",
    "        break\n",
    "\n",
    "    # Check if we have a final answer\n",
    "    if \"tool_calls\" not in assistant_message or not assistant_message[\"tool_calls\"]:\n",
    "        # Final answer\n",
    "        answer = assistant_message.get(\"content\", \"\")\n",
    "        print(f\"Agent completed with final answer ({len(answer)} chars)\")\n",
    "        break\n",
    "        \n",
    "    # Process tool calls\n",
    "    messages.append(assistant_message)\n",
    "\n",
    "    tool_calls = assistant_message.get(\"tool_calls\", [])\n",
    "    for tool_call in tool_calls:\n",
    "        function_name = tool_call.get(\"function\", {}).get(\"name\", \"\")\n",
    "        function_args_str = tool_call.get(\"function\", {}).get(\"arguments\", \"{}\")\n",
    "\n",
    "        try:\n",
    "            import json\n",
    "            function_args = json.loads(function_args_str)\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Invalid tool call arguments: {function_args_str}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Tool call: {function_name}({function_args})\")\n",
    "\n",
    "        # Execute tool using decoupled functions\n",
    "        if function_name == \"search_web\":\n",
    "            query = function_args.get(\"query\", \"\")\n",
    "            count = function_args.get(\"count\", 5)\n",
    "            tool_result = execute_search_web(query, count)\n",
    "        elif function_name == \"fetch_url\":\n",
    "            url = function_args.get(\"url\", \"\")\n",
    "            tool_result = execute_fetch_url(url, fetched_urls, DEFAULT_MAX_PAGES_FETCHED)\n",
    "        else:\n",
    "            tool_result = {\n",
    "                \"content\": f\"Unknown tool: {function_name}\",\n",
    "                \"success\": False,\n",
    "            }\n",
    "\n",
    "        # Add tool result to messages\n",
    "        messages.append({\n",
    "            \"role\": \"tool\",\n",
    "            \"tool_call_id\": tool_call.get(\"id\"),\n",
    "            \"content\": tool_result[\"content\"],\n",
    "        })\n",
    "\n",
    "        # Track sources and rejected URLs\n",
    "        if \"url\" in tool_result:\n",
    "            url = tool_result.get(\"final_url\") or tool_result.get(\"url\")\n",
    "            \n",
    "            if tool_result.get(\"success\"):\n",
    "                # Successful fetch\n",
    "                title = tool_result.get(\"title\", url)\n",
    "                \n",
    "                # Update provenance (simplified)\n",
    "                if url not in url_provenance:\n",
    "                    url_provenance[url] = {\n",
    "                        \"url\": url,\n",
    "                        \"title\": title,\n",
    "                        \"final_url\": tool_result.get(\"final_url\", url),\n",
    "                        \"canonical_url\": tool_result.get(\"canonical_url\"),\n",
    "                    }\n",
    "                \n",
    "                # Add to sources if not already present\n",
    "                if not any(s.get(\"url\") == url for s in sources):\n",
    "                    sources.append({\n",
    "                        \"url\": url,\n",
    "                        \"title\": title,\n",
    "                    })\n",
    "            else:\n",
    "                # Failed fetch - track as rejected\n",
    "                reason = tool_result.get(\"content\", \"Unknown error\")\n",
    "                status = tool_result.get(\"status\", \"unknown\")\n",
    "                rejected_urls.append({\n",
    "                    \"url\": url,\n",
    "                    \"reason\": f\"Status {status}: {reason}\",\n",
    "                })\n",
    "\n",
    "        # Track debug info\n",
    "        debug_traces.append({\n",
    "            \"step\": step + 1,\n",
    "            \"tool\": function_name,\n",
    "            \"args\": function_args,\n",
    "            \"result_length\": len(tool_result.get(\"content\", \"\")),\n",
    "            \"success\": tool_result.get(\"success\", False),\n",
    "        })\n",
    "\n",
    "\n",
    "\n",
    "# Construct final output\n",
    "final_output = construct_final_output(answer, user_prompt, sources, rejected_urls)\n",
    "\n",
    "# Pretty print the final output\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL OUTPUT:\")\n",
    "print(\"=\"*80)\n",
    "print(json.dumps(final_output, indent=2, ensure_ascii=False))\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b03d2a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"site:sovajewels.com кольцо белое золото керамика Smart Beautiful 110474820202\"\n",
    "out = brave_client.search(query, count=count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "bb830c79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Кольцо из белого золота и керамики Smart & Beautiful. Артикул: 110474820202 купить в интернет-магазине l SOVA Jewels',\n",
       "  'url': 'https://sovajewels.com/catalog/koltsa/koltso-iz-belogo-zolota-i-keramiki-smart-beautiful-artikul-110474820202.html',\n",
       "  'snippet': 'Купить Кольцо из белого золота и керамики Smart &amp; Beautiful. Артикул: 110474820202 по лучшей цене в интернет-магазине SOVA Jewels ⚜️Собственное производство✈️ Возможность быстрой доставки за 2-6 часов'},\n",
       " {'title': 'Купить Кольцо из белого золота и керамики Smart & Beautiful. Артикул: 110474820202 Киев | SOVA Jewels',\n",
       "  'url': 'https://sovajewels.com/catalog/koltsa/koltso-iz-belogo-zolota-i-keramiki-smart-beautiful-artikul-110474820202.html?OFFER_ID=28418',\n",
       "  'snippet': 'Если настроение диктует надеть решительный look прямо с подиума, Кольцо из белого золота и керамики Smart &amp; Beautiful. Артикул: 110474820202 - разумное и красивое дополнение.'},\n",
       " {'title': 'Кольцо из керамики с белым золотом Smart & Beautiful. Артикул: 110461520203 купить в интернет-магазине l SOVA Jewels',\n",
       "  'url': 'https://sovajewels.com/catalog/koltsa/Koltso-iz-keramiki-s-belym-zolotom-Smart-Beautiful-Artikul-110461520203.html',\n",
       "  'snippet': 'Купить Кольцо из керамики с белым золотом Smart &amp; Beautiful. Артикул: 110461520203 по лучшей цене в интернет-магазине SOVA Jewels ⚜️Собственное производство✈️ Возможность быстрой доставки за 2-6 часов'},\n",
       " {'title': 'Коллекция Smart & Beautiful | Ювелирные украшения от SOVA Jewels',\n",
       "  'url': 'https://sovajewels.com/collections/smart-beautiful/',\n",
       "  'snippet': 'Ювелирные украшения из коллекции Smart &amp; Beautiful – купить ювелирные изделия в интернет магазине. ✔️Всегда быстрая доставка | Украшения от производителя | Лучшая цена ➦ sovajewels.com | Номер телефона: ☎️: (800) 300 750'},\n",
       " {'title': 'Кольцо из белого золота с бриллиантами Smart & Beautiful. Артикул: 110461420201. Купить кольцо | SOVA Jewels',\n",
       "  'url': 'https://sovajewels.com/catalog/koltsa/koltso-iz-belogo-zolota-s-brilliantami-smart-beautiful-artikul-110461420201.html?OFFER_ID=37991',\n",
       "  'snippet': 'Smart &amp; Beautiful - это <strong>коллекция ювелирных украшений, вдохновленная интеллектуальной красотой</strong>. Ее отличительная черта — керамика в качестве ...'}]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5792c759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final output construction\n",
    "import json\n",
    "import re\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "def extract_json_from_text(text: str):\n",
    "    \"\"\"Try to extract JSON from text (handles markdown code blocks, etc.).\"\"\"\n",
    "    # Try to find JSON in code blocks\n",
    "    json_match = re.search(r'```(?:json)?\\s*(\\{.*?\\})\\s*```', text, re.DOTALL)\n",
    "    if json_match:\n",
    "        return json_match.group(1)\n",
    "    \n",
    "    # Try to find JSON object directly\n",
    "    json_match = re.search(r'\\{.*\\}', text, re.DOTALL)\n",
    "    if json_match:\n",
    "        return json_match.group(0)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def construct_final_output(answer: str, user_prompt: str, sources: list, rejected_urls: list):\n",
    "    \"\"\"Construct the final output JSON structure.\"\"\"\n",
    "    # Try to parse LLM's answer as JSON\n",
    "    parsed_output = None\n",
    "    try:\n",
    "        json_text = extract_json_from_text(answer)\n",
    "        parsed_output = json.loads(json_text)\n",
    "    except (json.JSONDecodeError, AttributeError) as e:\n",
    "        print(f\"Could not parse LLM output as JSON: {e}\")\n",
    "        parsed_output = None\n",
    "    \n",
    "    # Extract URLs from user prompt (first is reference, second is target)\n",
    "    all_urls = re.findall(r'https?://[^\\s]+', user_prompt)\n",
    "    reference_url = all_urls[0] if len(all_urls) > 0 else None\n",
    "    target_url = all_urls[1] if len(all_urls) > 1 else None\n",
    "    \n",
    "    # Extract target domain from target URL\n",
    "    target_domain = None\n",
    "    if target_url:\n",
    "        domain_match = re.search(r'https?://([^/\\s]+)', target_url)\n",
    "        if domain_match:\n",
    "            target_domain = domain_match.group(1)\n",
    "    \n",
    "    # Build final output structure\n",
    "    final_output = {\n",
    "        \"phase\": \"phase_1_2_definitions_to_categories\",\n",
    "        \"reference\": {\n",
    "            \"reference_url\": reference_url,\n",
    "            \"attributes\": parsed_output.get(\"reference\", {}).get(\"attributes\", {}) if parsed_output else {\n",
    "                \"product_type\": \"\",\n",
    "                \"brand\": \"\",\n",
    "                \"model\": \"\",\n",
    "                \"main_definitions\": [],\n",
    "                \"keywords\": [],\n",
    "                \"price_hint\": None,\n",
    "            },\n",
    "            \"uncertainty_notes\": parsed_output.get(\"reference\", {}).get(\"uncertainty_notes\", {}) if parsed_output else {\n",
    "                \"product_type\": \"\",\n",
    "                \"brand\": \"\",\n",
    "                \"model\": \"\",\n",
    "                \"main_definitions\": \"\",\n",
    "                \"keywords\": \"\",\n",
    "                \"price_hint\": \"\",\n",
    "            },\n",
    "            \"evidence_urls\": [s[\"url\"] for s in sources if s.get(\"url\")],\n",
    "        },\n",
    "        \"target_site\": {\n",
    "            \"domain\": target_domain or \"\",\n",
    "            \"language_prefix_hint\": \"\",\n",
    "        },\n",
    "        \"definition_groups\": parsed_output.get(\"definition_groups\", []) if parsed_output else [],\n",
    "        \"rejected\": rejected_urls,\n",
    "    }\n",
    "    \n",
    "    # If we have parsed output, merge it (but keep our evidence_urls and rejected)\n",
    "    if parsed_output:\n",
    "        if \"reference\" in parsed_output:\n",
    "            final_output[\"reference\"].update(parsed_output[\"reference\"])\n",
    "            # Ensure evidence_urls includes all sources\n",
    "            evidence_set = set(final_output[\"reference\"].get(\"evidence_urls\", []))\n",
    "            evidence_set.update([s[\"url\"] for s in sources if s.get(\"url\")])\n",
    "            final_output[\"reference\"][\"evidence_urls\"] = list(evidence_set)\n",
    "        \n",
    "        if \"target_site\" in parsed_output:\n",
    "            final_output[\"target_site\"].update(parsed_output[\"target_site\"])\n",
    "            if not final_output[\"target_site\"].get(\"domain\") and target_domain:\n",
    "                final_output[\"target_site\"][\"domain\"] = target_domain\n",
    "    \n",
    "    return final_output\n",
    "\n",
    "# Construct final output (only if we have an answer)\n",
    "if answer or sources:\n",
    "    final_output = construct_final_output(answer, user_prompt, sources, rejected_urls)\n",
    "    \n",
    "    # Pretty print the final output\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"FINAL OUTPUT:\")\n",
    "    print(\"=\"*80)\n",
    "    print(json.dumps(final_output, indent=2, ensure_ascii=False))\n",
    "    print(\"=\"*80)\n",
    "else:\n",
    "    print(\"\\nNo final output - agent did not complete or produce results.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968f04a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(messages[-1]['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5adef68d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15968a36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526a27a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
